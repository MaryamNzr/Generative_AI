{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run Llama 2 Locally with Python (Quickstart)\n",
    "\n",
    "This Jupyter Notebook is part of a Blog Post on https://swharden.com\n",
    "\n",
    "https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from models/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 6828.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The names of the days of the week, in order, are: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   580.29 ms\n",
      "llama_print_timings:      sample time =    24.99 ms /    36 runs   (    0.69 ms per token,  1440.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   580.26 ms /    16 tokens (   36.27 ms per token,    27.57 tokens per second)\n",
      "llama_print_timings:        eval time =  2374.99 ms /    35 runs   (   67.86 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =  3022.81 ms\n"
     ]
    }
   ],
   "source": [
    "# load the large language model file\n",
    "\n",
    "LLM = Llama(model_path=\"models/llama-2-7b-chat.ggmlv3.q8_0.bin\")\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"Q: What are the names of the days of the week? A:\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jupyter notebooks can be difficult to maintain for several reasons. Here are some of the common challenges that developers face when working with Jupyter notebooks:\n",
      "\n",
      "1. Lack of structure: Jupyter notebooks are flexible and allow users to organize their code in any way they like. However, this flexibility can also lead to a lack of structure, making it difficult to navigate and maintain the notebook over time.\n",
      "2. Multiple versions of code: Since Jupyter notebooks are dynamic and allow users to edit and run code interactively, it's easy for multiple versions of code to exist within a single notebook. This can make it challenging to track changes and maintain consistency across different sections of the notebook.\n",
      "3. Inconsistent formatting: Jupyter notebooks use Markdown syntax for formatting, which can lead to inconsistencies in formatting across different sections of the notebook. This can make it difficult to maintain a consistent look and feel throughout the notebook.\n",
      "4. Lack of version control: Since Jupyter notebooks are designed to be interactive, they don't have built-in version control systems like Git. This means that developers need to find alternative ways to manage their code and track changes over time.\n",
      "5. Difficulty in collaborating: Since Jupyter notebooks are designed for individual use, it can be challenging to collaborate with others on a single notebook. This can lead to conflicts and confusion when multiple people are working on the same notebook.\n",
      "6. Limited project structure: Jupyter notebooks don't have a built-in project structure, which means that developers need to find alternative ways to organize their code and dependencies. This can lead to difficulties in managing large projects and scaling up to more complex workflows.\n",
      "7. Difficulty in deploying: Deploying Jupyter notebooks can be challenging due to the lack of standardization across different environments and platforms. This can make it difficult to ensure that a notebook will work as expected in different settings.\n",
      "8. Limited support for data management: While Jupyter notebooks are great for exploratory data analysis, they don't have built-in support for large datasets or complex data processing tasks. This can make it difficult to manage and analyze large datasets within a single notebook.\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3983.67 ms\n",
      "llama_print_timings:      sample time =   359.94 ms /   496 runs   (    0.73 ms per token,  1378.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   379.48 ms /    13 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
      "llama_print_timings:        eval time = 34094.93 ms /   495 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time = 35494.21 ms\n"
     ]
    }
   ],
   "source": [
    "# create a text prompt\n",
    "prompt = \"Q: Why are Jupyter notebooks difficult to maintain? A:\"\n",
    "\n",
    "# set max_tokens to 0 to remove the response size limit\n",
    "output = LLM(prompt, max_tokens=0)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script creates a database of information gathered from local text files. \n",
    "\"\"\"\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# define what documents to load\n",
    "loader = DirectoryLoader(\"./\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                          chunk_overlap=50)\n",
    "texts = splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key results on page 3 of the PDF file are:\n",
      "\n",
      "* Total value of production (£ million)\n",
      "* Volume of output (thousand tons)\n",
      "* Value added (£ million)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script reads the database of information from local text files\n",
    "and uses a large language model to answer questions about their content.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "llm = CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "                    model_type='llama',\n",
    "                    config={'max_new_tokens': 256, 'temperature': 0.01})\n",
    "\n",
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])\n",
    "qa_llm = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})\n",
    "\n",
    "# ask the AI chat about information in our local files\n",
    "prompt = \"what is the key results from the pdf file?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purposes and uses of the PDF are to provide information on how the statistics are produced, managed, and used in accordance with the Statistics and Registration Service Act 2007 and the Code of Practice for Official Statistics. The PDF also explains how the data is collected, processed, and analyzed to meet identified user needs and to ensure that the statistics are well explained, readily accessible, produced according to sound methods, and managed impartially and objectively in the public interest.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \"give me information about Purposes and uses of the pdf\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The release includes data relating to delivery for the six-month period ending 30 September 2021, covering all current and historical programmes delivered by Homes England, including the acquisition of existing land or property as well as new house building, and some programmes provide a mix of affordable and market housing.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \"What is included in this release?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
